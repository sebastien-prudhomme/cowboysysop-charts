name: local-ai
productName: LocalAI
appVersion: 2.12.1
description: is <<a drop-in replacement REST API compatible with OpenAI API specifications for local inferencing>>
home: https://localai.io/
icon: https://user-images.githubusercontent.com/2420543/233147843-88697415-6dbf-4368-a862-ab217f9f7342.jpeg
license: 2023
source: https://github.com/go-skynet/LocalAI
version: 5.0.0
updates: |
  ### Upgrading to version 5.0.0

  Information about services are no more injected into pod's environment variable.

  ### Upgrading to version 4.0.0

 We’ve removed rwkv.cpp and bert.cpp

Deprecation of the exllama backend


As we continue to evolve, we are officially deprecating the gpt4all.cpp and petals backends


  The trust_remote_code setting in the YAML config file of the model are now consumed for enhanced security measures also for the AutoGPTQ and transformers backend, thanks to @dave-gray101's contribution (#1799). If your model relied on the old behavior and you are sure of what you are doing, set trust_remote_code: true in the YAML config file.

  backend supprimé, cf evoliton de https://github.com/mudler/LocalAI/blob/master/docs/content/docs/reference/compatibility-table.md
  = bloomz, dolly, falcon, falcon-ggml, gpt2, gptj, gptneox, mpt, replit, starcoder

  rename llama-stable to llama-ggml

  Faire le switch vers v2.12.1-ffmpeg-core (sans dépendances Python, cf la doc)

  Refaire un --help avec la v2.12.1

  ### Upgrading to version 3.0.0

  The llama backend now uses the GGUF model format. Migrate to the llama-stable backend to continue using the GGML model format.

  ### Upgrading to version 2.0.0

  The chart is now tested with Kubernetes >= 1.24 and Helm >= 3.9.

  Future upgrades may introduce undetected breaking changes if you continue to use older versions.
components:
  - name: local-ai
    configmap: |
      preload-models.json: |
        {{- .Values.config.preloadModels | toPrettyJson | nindent 4 }}
    deployment:
      type: deployment
      container:
        env: |
          - name: ADDRESS
            value: ":{{ .Values.containerPorts.http }}"
          - name: AUDIO_PATH
            value: /data/generated-audio
          - name: IMAGE_PATH
            value: /data/generated-images
          - name: MODELS_PATH
            value: /data/models
          - name: GALLERIES
            value: {{ .Values.config.galleries | toJson | quote }}
          - name: PRELOAD_MODELS_CONFIG
            value: /config/preload-models.json
        livenessProbe:
          httpGet:
            path: /healthz
        readinessProbe:
          httpGet:
            path: /readyz
        startupProbe:
          httpGet:
            path: /healthz
        volumeMounts: |
          - name: config
            mountPath: /config/preload-models.json
            subPath: preload-models.json
          - name: data
            mountPath: /data
    image:
      registry: quay.io
      repository: go-skynet/local-ai
      tag: v2.12.1-ffmpeg-core
    ingress:
      paths:
        - /
    persistentvolumeclaim: true
    service:
      ports:
        - name: http
          number: 8080
          description: HTTP
    extraValues: |
      config:
        ## @param config.galleries Model galleries
        galleries: []
          # - name: model-gallery
          #   url: github:go-skynet/model-gallery/index.yaml
          # - name: huggingface
          #   url: github:go-skynet/model-gallery/huggingface.yaml

        ## @param config.preloadModels Models to preload (enable and configure startup probe according to model download time)
        preloadModels: []
          # - id: model-gallery@text-embedding-ada-002
          # - url: github:go-skynet/model-gallery/gpt4all-j.yaml
          #   name: gpt-3.5-turbo
          # - id: model-gallery@stablediffusion
          # - id: model-gallery@whisper-1
          # - id: model-gallery@voice-en-us-kathleen-low
tests: |
  import requests


  def test_service_connection():
      url = "http://{{ include "local-ai.fullname" . }}:{{ .Values.service.ports.http }}/v1/models"

      response = requests.get(url)

      assert response.status_code == 200
