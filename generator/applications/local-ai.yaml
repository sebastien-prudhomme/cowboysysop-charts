name: local-ai
productName: LocalAI
appVersion: 1.22.0
description: is <<a drop-in replacement REST API compatible with OpenAI API specifications for local inferencing>>
home: https://localai.io/
icon: https://user-images.githubusercontent.com/2420543/233147843-88697415-6dbf-4368-a862-ab217f9f7342.jpeg
license: 2023
source: https://github.com/go-skynet/LocalAI
version: 2.1.0
updates: |
  ### Upgrading to version 2.0.0

  The chart is now tested with Kubernetes >= 1.24 and Helm >= 3.9.

  Future upgrades may introduce undetected breaking changes if you continue to use older versions.
components:
  - name: local-ai
    configmap: |
      preload-models.json: |
        {{- .Values.config.preloadModels | toPrettyJson | nindent 4 }}
    deployment:
      type: deployment
      initContainer:
        command: |
          - /bin/bash
          - -ec
          - |
            mkdir -p /data/generated-audio
            mkdir -p /data/generated-images
            mkdir -p /data/models
        volumeMounts: |
          - name: data
            mountPath: /data
      container:
        env: |
          - name: ADDRESS
            value: ":{{ .Values.containerPorts.http }}"
          - name: AUDIO_PATH
            value: /data/generated-audio
          - name: IMAGE_PATH
            value: /data/generated-images
          - name: MODELS_PATH
            value: /data/models
          - name: GALLERIES
            value: {{ .Values.config.galleries | toJson | quote }}
          - name: PRELOAD_MODELS_CONFIG
            value: /config/preload-models.json
        livenessProbe:
          httpGet:
            path: /healthz
        readinessProbe:
          httpGet:
            path: /readyz
        startupProbe:
          httpGet:
            path: /healthz
        volumeMounts: |
          - name: config
            mountPath: /config/preload-models.json
            subPath: preload-models.json
          - name: data
            mountPath: /data
    image:
      registry: quay.io
      repository: go-skynet/local-ai
      tag: v1.22.0-ffmpeg
    ingress: true
    persistentvolumeclaim: true
    service:
      ports:
        - name: http
          number: 8080
          description: HTTP
    extraValues: |
      config:
        ## @param config.galleries Model galleries
        galleries: []
          # - name: model-gallery
          #   url: github:go-skynet/model-gallery/index.yaml

        ## @param config.preloadModels Models to preload (enable and configure startup probe according to model download time)
        preloadModels: []
          # - id: model-gallery@text-embedding-ada-002
          # - url: github:go-skynet/model-gallery/gpt4all-j.yaml
          #   name: gpt-3.5-turbo
          # - id: model-gallery@stablediffusion
          # - id: model-gallery@whisper-1
          # - id: model-gallery@voice-en-us-kathleen-low
tests: |
  import requests


  def test_service_connection():
      url = "http://{{ include "local-ai.fullname" . }}:{{ .Values.service.ports.http }}/v1/models"

      response = requests.get(url)

      assert response.status_code == 200
